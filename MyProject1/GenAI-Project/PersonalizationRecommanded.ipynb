{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker instance\n",
    "fake = Faker()\n",
    "\n",
    "# Generate Customer Demographics Data\n",
    "def generate_customer_demographics(num_customers=1000):\n",
    "    customer_data = []\n",
    "    for _ in range(num_customers):\n",
    "        customer = {\n",
    "            'customer_id': fake.uuid4(),\n",
    "            'name': fake.name(),\n",
    "            'age': random.randint(18, 70),\n",
    "            'gender': random.choice(['Male', 'Female']),\n",
    "            'marital_status': random.choice(['Single', 'Married', 'Divorced']),\n",
    "            'education': random.choice(['High School', 'Bachelor', 'Master', 'PhD']),\n",
    "            'occupation': fake.job(),\n",
    "            'salary': random.randint(20000, 150000),  # Yearly salary\n",
    "        }\n",
    "        customer_data.append(customer)\n",
    "    return pd.DataFrame(customer_data)\n",
    "\n",
    "# Generate Customer Financial Behavior Data\n",
    "def generate_financial_behavior(customer_ids, num_records=2000):\n",
    "    financial_data = []\n",
    "    for _ in range(num_records):\n",
    "        product_type = random.choice(['Personal Loan', 'Home Loan', 'Credit Card'])\n",
    "        loan_amount = random.randint(5000, 500000) if product_type != 'Credit Card' else random.randint(5000, 150000)\n",
    "        credit_limit = random.randint(1000, 150000) if product_type == 'Credit Card' else None\n",
    "        utilization = random.uniform(0.1, 1.0) if product_type == 'Credit Card' else None\n",
    "        max_dpd = random.choice([0, 15, 30, 60, 90, 120])\n",
    "        default_status = random.choice([True, False])\n",
    "\n",
    "        financial_behavior = {\n",
    "            'customer_id': random.choice(customer_ids),\n",
    "            'product_type': product_type,\n",
    "            'loan_amount': loan_amount,\n",
    "            'credit_limit': credit_limit,\n",
    "            'credit_utilization': utilization,\n",
    "            'emi_paid': random.randint(1, 24),\n",
    "            'tenure_months': random.randint(12, 60),\n",
    "            'max_dpd': max_dpd,\n",
    "            'default_status': default_status\n",
    "        }\n",
    "        financial_data.append(financial_behavior)\n",
    "    return pd.DataFrame(financial_data)\n",
    "\n",
    "# Generate Customer Enquiries Data (Last 3 months)\n",
    "def generate_customer_enquiries(customer_ids, num_records=500):\n",
    "    enquiries_data = []\n",
    "    for _ in range(num_records):\n",
    "        product_type = random.choice(['Personal Loan', 'Home Loan', 'Credit Card'])\n",
    "        enquiry_amount = random.randint(5000, 500000) if product_type != 'Credit Card' else random.randint(5000, 100000)\n",
    "        enquiry = {\n",
    "            'customer_id': random.choice(customer_ids),\n",
    "            'enquiry_date': fake.date_between(start_date='-90d', end_date='today'),\n",
    "            'product_type': product_type,\n",
    "            'enquiry_amount': enquiry_amount,\n",
    "            'status': random.choice(['Approved', 'Rejected'])\n",
    "        }\n",
    "        enquiries_data.append(enquiry)\n",
    "    return pd.DataFrame(enquiries_data)\n",
    "\n",
    "# Generate Customer Transaction Data (Past 6 months)\n",
    "def generate_customer_transactions(customer_ids, num_records=5000):\n",
    "    transactions_data = []\n",
    "    for _ in range(num_records):\n",
    "        transaction_date = fake.date_between(start_date='-180d', end_date='today')\n",
    "        transaction_amount = random.uniform(50, 10000)\n",
    "        \n",
    "        # Transaction description with salary-related and hobby keywords\n",
    "        transaction_description = random.choice([\n",
    "            'Salary from XYZ Corp', 'Amazon Purchase', 'Grocery Store', 'Gym Membership',\n",
    "            'Netflix Subscription', 'Restaurant', 'Fuel Station', 'Travel Booking', \n",
    "            'SALARY - ABC Corp', 'SAL credited from DEF Ltd', 'Monthly Salary GHI Pvt Ltd', \n",
    "            'Rent Payment', 'Car Insurance', 'Mobile Phone Bill', 'Electricity Bill', 'Spotify Subscription',\n",
    "            'Uber Ride', 'Etsy Shopping', 'Concert Ticket', 'Books Purchase'\n",
    "        ])\n",
    "\n",
    "        # Salary detection\n",
    "        salary_keywords = ['Salary', 'SALARY', 'SAL', 'SAL credited', 'Monthly Salary']\n",
    "        is_salary = any(keyword in transaction_description.upper() for keyword in salary_keywords)\n",
    "\n",
    "        # Hobbies detection based on transaction descriptions\n",
    "        hobbies = None\n",
    "        if \"Amazon\" in transaction_description or \"Etsy\" in transaction_description:\n",
    "            hobbies = 'Shopping'\n",
    "        elif \"Netflix\" in transaction_description or \"Spotify\" in transaction_description:\n",
    "            hobbies = 'Entertainment'\n",
    "        elif \"Gym\" in transaction_description:\n",
    "            hobbies = 'Fitness'\n",
    "        elif \"Concert\" in transaction_description:\n",
    "            hobbies = 'Music'\n",
    "        elif \"Books\" in transaction_description:\n",
    "            hobbies = 'Reading'\n",
    "        elif \"Travel\" in transaction_description or \"Uber Ride\" in transaction_description:\n",
    "            hobbies = 'Travel'\n",
    "\n",
    "        transaction = {\n",
    "            'customer_id': random.choice(customer_ids),\n",
    "            'transaction_date': transaction_date,\n",
    "            'transaction_amount': transaction_amount,\n",
    "            'transaction_description': transaction_description,\n",
    "            'account_balance': random.uniform(500, 20000),\n",
    "            'is_salary': is_salary,\n",
    "            'hobby_detected': hobbies\n",
    "        }\n",
    "        transactions_data.append(transaction)\n",
    "\n",
    "    return pd.DataFrame(transactions_data)\n",
    "\n",
    "# Generate consistent data across all categories\n",
    "customers = generate_customer_demographics(5000)\n",
    "financial_behavior = generate_financial_behavior(customers['customer_id'], num_records=15000)\n",
    "enquiries = generate_customer_enquiries(customers['customer_id'], num_records=4000)\n",
    "transactions = generate_customer_transactions(customers['customer_id'], num_records=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Data for Clustering\n",
    "# Aggregating financial and transaction data by customer_id to create summary features\n",
    "financial_summary = financial_behavior.groupby('customer_id').agg({\n",
    "    'loan_amount': 'mean',\n",
    "    'credit_limit': 'mean',\n",
    "    'credit_utilization': 'mean',\n",
    "    'emi_paid': 'sum',\n",
    "    'tenure_months': 'mean',\n",
    "    'max_dpd': 'max',\n",
    "    'default_status': 'mean',\n",
    "    'product_type':'unique'\n",
    "}).reset_index()\n",
    "\n",
    "transaction_summary = transactions.groupby('customer_id').agg({\n",
    "    'transaction_amount': 'mean',\n",
    "    'account_balance': 'mean',\n",
    "    'is_salary': 'sum',\n",
    "    'transaction_amount': lambda x: x[transactions['is_salary'] == 1].sum()\n",
    "}).reset_index()\n",
    "\n",
    "# Assuming 'enquiries' has columns like ['customer_id', 'product_type', 'enquiry_date', 'enquiry_amount']\n",
    "enquiries_summary = enquiries.groupby('customer_id').agg({\n",
    "    'enquiry_amount': 'mean',  # Average amount enquired\n",
    "    'product_type': lambda x: x.nunique(),  # Number of unique products enquired\n",
    "    'customer_id': 'count'  # Total number of enquiries\n",
    "}).rename(columns={\n",
    "    'customer_id': 'total_enquiries',\n",
    "    'product_type': 'unique_products_enquired'\n",
    "}).reset_index()\n",
    "\n",
    "# Merging demographics, financial behavior, and transaction summary\n",
    "merged_data = pd.merge(customers, financial_summary, on='customer_id', how='left')\n",
    "merged_data = pd.merge(merged_data, enquiries_summary, on='customer_id', how='left')\n",
    "merged_data = pd.merge(merged_data, transaction_summary, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explode the list in 'product_type' column\n",
    "df_exploded = merged_data.explode('product_type')\n",
    "\n",
    "# Step 2: One-hot encode the 'product_type' column\n",
    "df_encoded = pd.get_dummies(df_exploded['product_type'])\n",
    "\n",
    "merged_data = pd.concat([df_exploded, df_encoded], axis=1)\n",
    "\n",
    "# Step 4: Group by the original index and aggregate to bring it back into one row per customer\n",
    "df_final = merged_data.groupby(merged_data.index).sum()\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation function for each column\n",
    "aggregation_functions = {\n",
    "    'customer_id': 'first',  # Keep first occurrence (assuming it's the same for the group)\n",
    "    'name': 'first',         # Keep the first name in each group\n",
    "    'age': 'mean',           # For age, you can take the average or median\n",
    "    'gender': 'first',       # Assuming gender is the same within each group, take the first\n",
    "    'marital_status': 'first', # Same for marital status\n",
    "    'education': 'first',    # Same for education\n",
    "    'occupation': 'first',   # Same for occupation\n",
    "    'salary': 'sum',         # Sum numerical values like salary\n",
    "    'loan_amount': 'sum',    # Sum numerical values like loan amount\n",
    "    'credit_limit': 'sum',   # Sum numerical values like credit limit\n",
    "    'credit_utilization': 'sum',\n",
    "    'emi_paid':'sum',\n",
    "    'tenure_months':'sum',\n",
    "    'max_dpd':'max',\n",
    "    'default_status':'max',\n",
    "    'enquiry_amount': 'sum',\n",
    "    'unique_products_enquired': 'sum',\n",
    "    'total_enquiries': 'sum',\n",
    "    'transaction_amount': 'sum',\n",
    "    'account_balance': 'sum',\n",
    "    'is_salary': 'mean',     # For boolean-like columns, you can take the mean (0 or 1)\n",
    "    'Credit Card': 'max',    # For categorical (binary) features, take max (0 or 1)\n",
    "    'Home Loan': 'max',\n",
    "    'Personal Loan': 'max',\n",
    "}\n",
    "\n",
    "# Group by and apply aggregation functions\n",
    "df_final = merged_data.groupby(merged_data.index).agg(aggregation_functions)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns for clustering\n",
    "clustering_data = df_final.drop(columns=['customer_id', 'name', 'occupation','gender','marital_status','education'])\n",
    "print(clustering_data.columns)\n",
    "# Handle missing values\n",
    "clustering_data.fillna(0, inplace=True)\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(clustering_data)\n",
    "\n",
    "# Determine optimal number of clusters (elbow method)\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(range(1, 11), inertia)\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal K (say K=4 based on the elbow curve)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df_final['customer_segment'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# View segmented data\n",
    "print(df_final[['customer_id', 'customer_segment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# Assuming df_final is your DataFrame and 'customer_segment' is the column to group by\n",
    "float_columns = df_final.select_dtypes(include=['int','float'])\n",
    "grouped_df = float_columns.groupby('customer_segment')\n",
    "\n",
    "# Summary of each segment\n",
    "segment_description = grouped_df.describe()\n",
    "segment_description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Descriptions:\n",
    "1. **Segment 1: Affluent** :\n",
    "Higher average salary, loan amount, credit limit, and transaction amounts. These customers have higher financial standing with substantial balances and engagement in high-value transactions.\n",
    "\n",
    "2. **Segment 0: Cautious** :\n",
    "Lower loan amounts and credit utilization, fewer enquiries, and lower transaction amounts. This segment seems to manage smaller finances and be more risk-averse or conservative in their credit usage.\n",
    "\n",
    "3. **Segment 2: Moderate** :\n",
    "Mid-range loan amounts, credit limits, and financial activity. They display a balanced financial profile with moderate usage across different financial metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Pricing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for regression model (predicting loan amount)\n",
    "# Drop non-numerical and unnecessary columns for regression\n",
    "regression_data = df_final.drop(columns=['customer_id', 'name', 'occupation','gender','marital_status','education'])\n",
    "regression_data.fillna(-888, inplace=True)\n",
    "\n",
    "# Define models for each product type\n",
    "rf_regressor_personal_loan = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor_home_loan = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor_credit_card = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model for Personal Loan\n",
    "X_personal_loan = regression_data[(regression_data['Home Loan'] == 0) & (regression_data['Credit Card']== 0) & (regression_data['Personal Loan']== 1)].drop(columns=['loan_amount'])\n",
    "y_personal_loan = regression_data[(regression_data['Home Loan'] == 0) & (regression_data['Credit Card']== 0) & (regression_data['Personal Loan']== 1)]['loan_amount']\n",
    "X_train_personal_loan, X_test_personal_loan, y_train_personal_loan, y_test_personal_loan = train_test_split(X_personal_loan, y_personal_loan, test_size=0.2, random_state=42)\n",
    "rf_regressor_personal_loan.fit(X_personal_loan, y_personal_loan)\n",
    "\n",
    "# Train the model for Home Loan\n",
    "X_home_loan = regression_data[(regression_data['Home Loan'] == 1) & (regression_data['Credit Card']== 0) & (regression_data['Personal Loan']== 0)].drop(columns=['loan_amount'])\n",
    "y_home_loan = regression_data[(regression_data['Home Loan'] == 1) & (regression_data['Credit Card']== 0) & (regression_data['Personal Loan']== 0)]['loan_amount']\n",
    "X_train_home_loan, X_test_home_loan, y_train_home_loan, y_test_home_loan = train_test_split(X_home_loan, y_home_loan, test_size=0.2, random_state=42)\n",
    "rf_regressor_home_loan.fit(X_train_home_loan, y_train_home_loan)\n",
    "\n",
    "# Train the model for Credit Card\n",
    "X_credit_card = regression_data[(regression_data['Home Loan'] == 0) & (regression_data['Credit Card']== 1) & (regression_data['Personal Loan']== 0)].drop(columns=['loan_amount'])\n",
    "y_credit_card = regression_data[(regression_data['Home Loan'] == 0) & (regression_data['Credit Card']== 1) & (regression_data['Personal Loan']== 0)]['loan_amount']\n",
    "X_train_credit_card, X_test_credit_card, y_train_credit_card, y_test_credit_card = train_test_split(X_credit_card, y_credit_card, test_size=0.2, random_state=42)\n",
    "rf_regressor_credit_card.fit(X_train_credit_card, y_train_credit_card)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = rf_regressor_credit_card.predict(X_test_credit_card)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test_credit_card, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Example: Making predictions for a new customer\n",
    "new_customer_data = X_test_credit_card.iloc[0:1]  # Select one customer data for testing\n",
    "predicted_loan_amount = rf_regressor_credit_card.predict(new_customer_data)\n",
    "print(f\"Predicted Loan Amount: {predicted_loan_amount[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_final is already defined\n",
    "# Preparing data for classification model\n",
    "classification_data = df_final.copy()\n",
    "classification_data.fillna(-888, inplace=True)\n",
    "# Step 2: Drop unnecessary columns for the model and target columns\n",
    "X_classification = classification_data.drop(columns=['customer_id', 'name', 'occupation', 'gender', \n",
    "                                                     'marital_status', 'education', 'Credit Card', 'Home Loan', 'Personal Loan'])\n",
    "y_classification = classification_data[['Credit Card', 'Home Loan', 'Personal Loan']]  # Multi-output target columns\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_classification, y_classification, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize Random Forest Classifier with MultiOutputClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_classifier = MultiOutputClassifier(rf_classifier)\n",
    "\n",
    "# Step 5: Train the model on the training data for all three outputs\n",
    "multi_target_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Step 6: Predict probabilities for the test set\n",
    "y_prob_test = [estimator.predict_proba(X_test_class)[:, 1] for estimator in multi_target_classifier.estimators_]\n",
    "\n",
    "# Combine probabilities into a DataFrame for easier handling\n",
    "probabilities = pd.DataFrame({\n",
    "    'Credit Card': y_prob_test[0],   # Probability for Credit Card\n",
    "    'Home Loan': y_prob_test[1],     # Probability for Home Loan\n",
    "    'Personal Loan': y_prob_test[2]  # Probability for Personal Loan\n",
    "})\n",
    "\n",
    "# Step 7: Find the most probable product for each customer\n",
    "most_probable_product = probabilities.idxmax(axis=1)\n",
    "\n",
    "# Optional: Evaluate model performance\n",
    "y_pred = multi_target_classifier.predict(X_test_class)\n",
    "print(\"\\nClassification Report for Credit Card:\")\n",
    "print(classification_report(y_test_class['Credit Card'], y_pred[:, 0]))\n",
    "\n",
    "print(\"\\nClassification Report for Home Loan:\")\n",
    "print(classification_report(y_test_class['Home Loan'], y_pred[:, 1]))\n",
    "\n",
    "print(\"\\nClassification Report for Personal Loan:\")\n",
    "print(classification_report(y_test_class['Personal Loan'], y_pred[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index(['customer_id', 'name', 'age', 'gender', 'marital_status', 'education',\n",
    "#        'occupation', 'salary', 'loan_amount', 'credit_limit',\n",
    "#        'credit_utilization', 'emi_paid', 'tenure_months', 'max_dpd',\n",
    "#        'default_status', 'product_type', 'enquiry_amount',\n",
    "#        'unique_products_enquired', 'total_enquiries', 'transaction_amount',\n",
    "#        'account_balance', 'is_salary', 'customer_segment'],\n",
    "#       dtype='object')\n",
    "\n",
    "X_classification.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the file paths for saving\n",
    "model_paths = {\n",
    "    'kmeans': 'E:/Abinash/2025/MyProject/models/kmeans_model.pkl',\n",
    "    'scaler': 'E:/Abinash/2025/MyProject/models/scaler.pkl',\n",
    "    'multi_target_classifier': 'E:/Abinash/2025/MyProject/models/multi_target_classifier_model.pkl',\n",
    "    'rf_regressor_personal_loan': 'E:/Abinash/2025/MyProject/models/rf_regressor_personal_loan_model.pkl',\n",
    "    'rf_regressor_home_loan': 'E:/Abinash/2025/MyProject/models/rf_regressor_home_loan_model.pkl',\n",
    "    'rf_regressor_credit_card': 'E:/Abinash/2025/MyProject/models/rf_regressor_credit_card_model.pkl'\n",
    "}\n",
    "\n",
    "# Save models to disk\n",
    "joblib.dump(kmeans, model_paths['kmeans'])\n",
    "joblib.dump(scaler, model_paths['scaler'])\n",
    "joblib.dump(multi_target_classifier, model_paths['multi_target_classifier'])\n",
    "joblib.dump(rf_regressor_personal_loan, model_paths['rf_regressor_personal_loan'])\n",
    "joblib.dump(rf_regressor_home_loan, model_paths['rf_regressor_home_loan'])\n",
    "joblib.dump(rf_regressor_credit_card, model_paths['rf_regressor_credit_card'])\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    'kmeans': 'E:/Abinash/2025/MyProject/models/kmeans_model.pkl',\n",
    "    'scaler': 'E:/Abinash/2025/MyProject/models/scaler.pkl',\n",
    "    'multi_target_classifier': 'E:/Abinash/2025/MyProject/models/multi_target_classifier_model.pkl',\n",
    "    'rf_regressor_personal_loan': 'E:/Abinash/2025/MyProject/models/rf_regressor_personal_loan_model.pkl',\n",
    "    'rf_regressor_home_loan': 'E:/Abinash/2025/MyProject/models/rf_regressor_home_loan_model.pkl',\n",
    "    'rf_regressor_credit_card': 'E:/Abinash/2025/MyProject/models/rf_regressor_credit_card_model.pkl'\n",
    "}\n",
    "\n",
    "kmeans = joblib.load(model_paths['kmeans'])\n",
    "scaler = joblib.load(model_paths['scaler'])\n",
    "multi_target_classifier = joblib.load(model_paths['multi_target_classifier'])\n",
    "rf_regressor_personal_loan = joblib.load(model_paths['rf_regressor_personal_loan'])\n",
    "rf_regressor_home_loan = joblib.load(model_paths['rf_regressor_home_loan'])\n",
    "rf_regressor_credit_card = joblib.load(model_paths['rf_regressor_credit_card'])\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "\n",
    "\n",
    "def process_customer_data(json_data, scaler):\n",
    "    # Convert JSON data to a DataFrame\n",
    "    customer_data = pd.DataFrame([json_data])\n",
    "    \n",
    "    # Drop columns not needed for clustering (based on your clustering model)\n",
    "    clustering_data = customer_data\n",
    "    \n",
    "    # Handle missing values by replacing with 0 (or any appropriate strategy)\n",
    "    clustering_data.fillna(0, inplace=True)\n",
    "    \n",
    "    scaled_data = scaler.transform(clustering_data)\n",
    "    \n",
    "    return customer_data, scaled_data\n",
    "\n",
    "def predict_customer_segment(scaled_data, kmeans):\n",
    "    # Predict the customer segment using your pre-trained KMeans model\n",
    "    customer_segment = kmeans.predict(scaled_data)\n",
    "    return customer_segment[0]\n",
    "\n",
    "def recommend_product_and_loan(json_data, kmeans, scaler, multi_target_classifier, rf_regressor_personal_loan, rf_regressor_home_loan, rf_regressor_credit_card):\n",
    "    # Convert JSON data to a DataFrame and scale the data\n",
    "    customer_data, scaled_data = process_customer_data(json_data, scaler)\n",
    "\n",
    "    # Step 1: Predict customer segment using KMeans\n",
    "    customer_segment = predict_customer_segment(scaled_data, kmeans)\n",
    "    \n",
    "    # Add the predicted customer segment back to the customer_data DataFrame\n",
    "    customer_data['customer_segment'] = customer_segment\n",
    "    print(customer_data.columns)\n",
    "    # Prepare for product recommendation using Random Forest Classifier\n",
    "    X_classification_prod = customer_data.drop(columns=['Credit Card', 'Home Loan', 'Personal Loan'])\n",
    "    X_classification_amt = customer_data\n",
    "    \n",
    "    # Step 2: Predict probabilities for each product using the multi-output classifier\n",
    "    prob_credit_card = [estimator.predict_proba(X_classification_prod)[:, 1] for estimator in multi_target_classifier.estimators_]\n",
    "\n",
    "    # Combine probabilities into a Series\n",
    "    product_probabilities = pd.Series({\n",
    "        'Credit Card': prob_credit_card[0][0],  # Since it's for one customer, we get the first value\n",
    "        'Home Loan': prob_credit_card[1][0],\n",
    "        'Personal Loan': prob_credit_card[2][0]\n",
    "    })\n",
    "\n",
    "    print(product_probabilities)\n",
    "    \n",
    "    # Identify the most probable product\n",
    "    recommended_product = product_probabilities.idxmax()    \n",
    "    recommended_probability = product_probabilities.max()\n",
    "\n",
    "    recommendation = f\"Recommended Product: {recommended_product} (Probability: {recommended_probability:.2f})\"\n",
    "\n",
    "    # Step 3: Predict loan amounts or credit limits based on the recommended product\n",
    "    if recommended_product == 'Personal Loan':\n",
    "        predicted_loan_amount_personal = rf_regressor_personal_loan.predict(X_classification_amt.drop(columns=['loan_amount']))\n",
    "        recommendation += f\"\\nPredicted Loan Amount: {predicted_loan_amount_personal[0]:,.2f}\"\n",
    "    \n",
    "    elif recommended_product == 'Home Loan':\n",
    "        predicted_loan_amount_home = rf_regressor_home_loan.predict(X_classification_amt.drop(columns=['loan_amount']))  \n",
    "        recommendation += f\"\\nPredicted Loan Amount: {predicted_loan_amount_home[0]:,.2f}\"\n",
    "    \n",
    "    elif recommended_product == 'Credit Card':\n",
    "        predicted_credit_limit = rf_regressor_credit_card.predict(X_classification_amt.drop(columns=['loan_amount']))\n",
    "        recommendation += f\"\\nPredicted Credit Limit: {predicted_credit_limit[0]:,.2f}\"\n",
    "    \n",
    "    if recommended_probability < 0.5:\n",
    "        recommendation = \"No suitable product recommendations found for this customer.\"\n",
    "    \n",
    "    return recommendation, customer_segment, product_probabilities\n",
    "\n",
    "# Example customer JSON data\n",
    "example_json = {\n",
    "    'age': 35, 'salary': 50000, 'loan_amount': 0, 'credit_limit': 10000, \n",
    "    'credit_utilization': 0.3, 'emi_paid': 1000, 'tenure_months': 12, \n",
    "    'max_dpd': 30, 'default_status': 0, 'enquiry_amount': 1000, \n",
    "    'unique_products_enquired': 3, 'total_enquiries': 5, \n",
    "    'transaction_amount': 5000, 'account_balance': 15000, 'is_salary': 1,\n",
    "    'Credit Card': 1, 'Home Loan': 0, 'Personal Loan': 0, \n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "recommend_product_and_loan(example_json, kmeans, scaler, multi_target_classifier, rf_regressor_personal_loan, rf_regressor_home_loan, rf_regressor_credit_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the LLM to generate Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import langchain_huggingface\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_model():\n",
    "    model_name = \"KingNish/Qwen2.5-0.5b-Test-ft\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to generate insights based on customer data\n",
    "# Function to generate insights based on customer data\n",
    "def generate_insights(customer_data):\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    # Create a prompt from the customer data\n",
    "    prompt = f\"\"\"\n",
    "    Generate a personalized summarised insight about the following customer based on their data:\n",
    "    \n",
    "    - Name: {customer_data['name']}\n",
    "    - Age: {customer_data['age']}\n",
    "    - Gender: {customer_data['gender']}\n",
    "    - Marital Status: {customer_data['marital_status']}\n",
    "    - Education: {customer_data['education']}\n",
    "    - Occupation: {customer_data['occupation']}\n",
    "    - Salary: ${customer_data['salary']:,.2f}\n",
    "    - Loan Amount: ${customer_data['loan_amount']:,.2f}\n",
    "    - Credit Limit: ${customer_data['credit_limit']:,.2f}\n",
    "    - Credit Utilization: {customer_data['credit_utilization']:.2%}\n",
    "    - EMI Paid: {customer_data['emi_paid']}\n",
    "    - Tenure Months: {round(float(customer_data['tenure_months']),2)}\n",
    "    - Max DPD: {customer_data['max_dpd']}\n",
    "    - Default Status: {int(customer_data['default_status'])}\n",
    "    - Account Balance: ${customer_data['account_balance']:,.2f}\n",
    "\n",
    "    Here are the Summarised Insights about {customer_data['name']}:\n",
    "    \"\"\"\n",
    "    # Initialize the query pipeline with increased max_length\n",
    "    query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        #max_length=6000,  # Increase max_length\n",
    "        truncation=True,\n",
    "        max_new_tokens=500,  # Control the number of new tokens generated\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "    insights = llm(prompt)\n",
    "    \n",
    "    return insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def df_row_to_json(row):\n",
    "    data = row.to_dict()\n",
    "    for key, value in data.items():\n",
    "        if pd.isnull(value):\n",
    "            data[key] = None\n",
    "        elif isinstance(value, pd.Timestamp):\n",
    "            data[key] = value.isoformat()\n",
    "\n",
    "    # Convert to JSON string\n",
    "    json_str = json.dumps(data, indent=4)\n",
    "    return json_str\n",
    "\n",
    "# Assuming `merged_data` is a DataFrame and we're converting row 8 to JSON\n",
    "json_data_str = df_row_to_json(df_final.iloc[6])\n",
    "\n",
    "# Now convert the JSON string back to a dictionary to access fields\n",
    "json_data_dict = json.loads(json_data_str)\n",
    "\n",
    "insights = generate_insights(json_data_dict)\n",
    "print(json_data_str)\n",
    "pd.DataFrame.from_dict(json_data_dict, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean unwanted characters and extract the insight\n",
    "def clean_and_extract_insight(insights):\n",
    "    # Remove unwanted characters (non-alphanumeric characters except spaces)\n",
    "    cleaned_insight = re.sub(r'[^a-zA-Z0-9\\s]', '', insights)\n",
    "    \n",
    "    # Extract the portion after \"Summarised Insight\"\n",
    "    if \"Here are the Summarised Insights about\" in cleaned_insight:\n",
    "        extracted_insight = cleaned_insight.split(\"Here are the Summarised Insights about\")[1].strip().split(\"\\n\\n\")[0]\n",
    "    else:\n",
    "        extracted_insight = cleaned_insight.strip()\n",
    "\n",
    "    return extracted_insight\n",
    "\n",
    "# Example usage\n",
    "cleaned_insight = clean_and_extract_insight(insights)\n",
    "print(cleaned_insight.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cleaned_insight.split(\"\\n\")[1:]\n",
    "all_data = list()\n",
    "for line in data:\n",
    "    all_data.append(line.strip())\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
